---
title: API de Decahose
keywords: ["Decahose", "flujo del 10 %", "API de decahose", "enterprise decahose", "flujo de muestra del 10 %", "streaming de Enterprise"]
---

<Note>
  Este endpoint se ha actualizado para incluir metadatos de edición de Publicaciones. Obtén más información sobre estos metadatos en la página de conceptos básicos de [&quot;Editar Publicaciones&quot;](/es/x-api/enterprise-gnip-2.0/fundamentals/edit-tweets). 
</Note>

### Flujo Decahose

[Enterprise](https://developer.x.com/en/products/x-api/enterprise)

*Esta es una API Enterprise disponible solo dentro de nuestros niveles de acceso gestionados. Para usar esta API, primero debes abrir una cuenta con nuestro equipo de ventas Enterprise. [Más información](https://developer.x.com/en/products/x-api/enterprise)*

Decahose entrega una muestra aleatoria del 10% del Firehose de X en tiempo real a través de una conexión de streaming. Esto se logra mediante un algoritmo de muestreo en tiempo real que selecciona los datos de forma aleatoria, a la vez que permite la entrega de datos con la baja latencia esperada a medida que X los envía a través del Firehose.

A continuación se muestran algunas de las características disponibles con Decahose:

* **URL ampliadas y mejoradas:** - desarrolla por completo las URL acortadas y proporciona metadatos adicionales (título y descripción de la página)
* **Particionamiento del flujo** - 2 particiones, cada una con el 50% del volumen del flujo Decahose
* **Fiabilidad mejorada** - diversidad geográfica de los sistemas backend

Nota: Estos datos se entregan de forma masiva y no admiten filtrado adicional (por ejemplo, por palabras clave).

`ENTERPRISE`

<div id="streaming-likes">
  ### Streaming de Me gusta
</div>

*Esta es una API Enterprise disponible únicamente dentro de nuestros niveles de acceso gestionado. Para usar esta API, primero debes configurar una cuenta con nuestro equipo de ventas Enterprise. [Más información](https://developer.x.com/en/products/x-api/enterprise)*

Los Me gusta permiten obtener información sobre quién indica que le gustan las Publicaciones y proporcionan recuentos precisos de Me gusta. Firehose y Decahose de Gnip pueden entregar Me gusta públicos relacionados con las Publicaciones entregadas vía Gnip. Esto produce métricas de participación pública y de audiencia en tiempo real asociadas a una Publicación.
 

**Introducción a los Me gusta**

Al prepararte para consumir datos de Me gusta, debes saber que:

* Los Me gusta se entregan a través de un stream independiente y separado
* Históricamente, los Me gusta se conocen como “Favorites”. La carga útil enriquecida en formato nativo mantiene esta nomenclatura
* Los streams incluyen solo Me gusta públicos
  * Público significa que el usuario que indica Me gusta, el creador de la Publicación y la Publicación son públicos en la plataforma
* Los Me gusta son muy similares a los Retweets y representan una señal pública de interacción
* Los elementos de la carga útil incluyen:
  * Objeto de Publicación original
  * Objeto de actor que creó la Publicación original
  * Objeto de actor que realizó la acción de Me gusta
* Solo se puede indicar Me gusta en contenido original
  * No se puede indicar Me gusta en Retweets. Un Me gusta a un Retweet se aplica a la Publicación original
  * Es *posible* indicar Me gusta a las Publicaciones citadas
* Las actividades de Me gusta incluyen los Gnip Enrichments aplicables (cuando se han adquirido/aplicado)
* Productos / Funciones compatibles
  * Los streams de Me gusta son compatibles con Backfill (cuando se ha adquirido/aplicado)
  * No hay compatibilidad con Replay para los streams de Me gusta
  * No hay compatibilidad con Search ni Historical para los Me gusta
  * No hay planes inmediatos para añadir compatibilidad de Me gusta a PowerTrack

**Decahose**

* Para la muestra del 10% de Publicaciones entregadas en Decahose, el stream incluye el 100% de los Me gusta públicos aplicables
* **Particiones:** 2
* **Estructura de URL**
  * https://gnip-stream.x.com/stream/sample10-likes/accounts/&lt;accountName&gt;/publishers/twitter/&lt;streamLabel&gt;.json?partition=1

**Carga útil en formato nativo enriquecido**

```json
{
   "id":"43560406e0ad9f68374445f5f30c33fc",
   "created_at":"Thu Dec 01 22:27:39 +0000 2016",
   "timestamp_ms":1480631259353,
   "favorited_status":{
      "created_at":"Thu Dec 01 22:27:16 +0000 2016",
      "id":804451830033948672,
      "id_str":"804451830033948672",
      "text":"@kafammheppduman",
      "source":"\u003ca href=\"http:\/\/x.com\/download\/android\" rel=\"nofollow\"\u003eX para Android\u003c\/a\u003e",
      "truncated":false,
      "in_reply_to_status_id":803694205163814912,
      "in_reply_to_status_id_str":"803694205163814912",
      "in_reply_to_user_id":2855759795,
      "in_reply_to_user_id_str":"2855759795",
      "in_reply_to_screen_name":"kafammheppduman",
      "user":{
         "id":2855759795,
         "id_str":"2855759795",
         "name":"delirdim kanka",
         "screen_name":"kafammheppduman",
         "location":"sanane",
         "url":"http:\/\/instagram.com\/kafammheppduman",
         "description":"Manit @GalatasaraySk \ud83d\udc9e",
         "translator_type":"none",
         "protected":false,
         "verified":false,
         "followers_count":3702,
         "friends_count":607,
         "listed_count":1,
         "favourites_count":113338,
         "statuses_count":389,
         "created_at":"Sat Nov 01 22:38:25 +0000 2014",
         "utc_offset":null,
         "time_zone":null,
         "geo_enabled":true,
         "lang":"tr",
         "contributors_enabled":false,
         "is_translator":false,
         "profile_background_color":"C0DEED",
         "profile_background_image_url":"",
         "profile_background_image_url_https":"",
         "profile_background_tile":false,
         "profile_link_color":"1DA1F2",
         "profile_sidebar_border_color":"C0DEED",
         "profile_sidebar_fill_color":"DDEEF6",
         "profile_text_color":"333333",
         "profile_use_background_image":true,
       "Profile_image_url": "http:\/\/pbs.twimg.com\/profile_images\/804421763945861121\/v3bp9pnq_normal.jpg",
         "Profile_image_url_https": "https:\/\/pbs.twimg.com\/profile_images\/804421763945861121\/v3bp9pnq_normal.jpg",
         "profile_banner_url":"https:\/\/pbs.twimg.com\/profile_banners\/2855759795\/1480630085",
         "default_profile":true,
         "default_profile_image":false,
         "following":null,
         "follow_request_sent":null,
         "notifications":null
      },
      "geo":null,
      "coordinates":null,
      "place":null,
      "contributors":null,
      "is_quote_status":false,
      "retweet_count":0,
      "favorite_count":0,
      "entities":{
         "hashtags":[],
         "urls":[],
         "user_mentions":[
            {
               "screen_name":"kafammheppduman",
               "name":"delirdim kanka",
               "id":2855759795,
               "id_str":"2855759795",
               "indices":[
                  0,
                  16
               ]
            }
         ],
         "symbols":[]
      },
      "favorited":false,
      "retweeted":false,
      "filter_level":"low",
      "lang":"und"
   },
   "user":{
      "id":774146932365070336,
      "id_str":"774146932365070336",
      "name":"Uyuyan Adam",
      "screen_name":"saykoMenn",
      "location":"Tarsus, T\u00fcrkiye",
      "url":"http:\/\/connected2.me\/pmc1i",
      "description":null,
      "translator_type":"none",
      "protected":false,
      "verified":false,
      "followers_count":414,
      "friends_count":393,
      "listed_count":0,
      "favourites_count":9868,
      "statuses_count":370,
      "created_at":"Fri Sep 09 07:26:26 +0000 2016",
      "utc_offset":null,
      "time_zone":null,
      "geo_enabled":false,
      "lang":"tr",
      "contributors_enabled":false,
      "is_translator":false,
      "profile_background_color":"F5F8FA",
      "profile_background_image_url":"",
      "profile_background_image_url_https":"",
      "profile_background_tile":false,
      "profile_link_color":"1DA1F2",
      "profile_sidebar_border_color":"C0DEED",
      "profile_sidebar_fill_color":"DDEEF6",
      "profile_text_color":"333333",
      "profile_use_background_image":true,
      "Profile_image_url": "http:\/\/pbs.twimg.com\/profile_images\/802992813424201728\/VMzcTL3x_normal.jpg",
      "Profile_image_url_https": "https:\/\/pbs.twimg.com\/profile_images\/802992813424201728\/VMzcTL3x_normal.jpg",
      "profile_banner_url":"https:\/\/pbs.twimg.com\/profile_banners\/774146932365070336\/1480283382",
      "default_profile":true,
      "default_profile_image":false,
      "following":null,
      "follow_request_sent":null,
      "notifications":null
   }
}
```

**Carga útil de eliminación de Me gusta / “Ya no me gusta”**

```json
{
   "delete":{
      "favorite":{
         "tweet_id":696615514970279937,
         "tweet_id_str":"696615514970279937",
         "user_id":2510287578,
         "user_id_str":"2510287578"
      },
      "timestamp_ms":"1480437031205"
   }
}
```

<div id="guides">
  ## Guías
</div>

<div id="recovery-and-redundency">
  ### Recovery and Redundency
</div>

**Introducción** 

Con la transmisión de grandes volúmenes de Publicaciones en tiempo real viene un conjunto de prácticas recomendadas que fomentan tanto la confiabilidad de los datos como su fidelidad total. Al consumir datos en tiempo real, maximizar el tiempo de conexión es un objetivo fundamental. Cuando se producen desconexiones, es importante detectarlas automáticamente y reconectarse. Después de reconectarse, es importante evaluar si hay períodos para los que se deba completar o recuperar datos faltantes. El componente que gestiona estos detalles y consume Publicaciones en tiempo real es solo una parte de un sistema con consideraciones de red, almacén de datos, servidor y almacenamiento. Dada la complejidad de estos sistemas, otra práctica recomendada es disponer de diferentes entornos de streaming, con al menos flujos separados para desarrollo/pruebas y producción.

Decahose incluye un conjunto de características que ayudan en este sentido.

1. Para admitir múltiples entornos, podemos implementar [Additional Streams](#AdditionalStreams) para tu cuenta. Estos streams son independientes entre sí y tienen una stream&#95;label diferente para ayudar a diferenciarlos.
2. Para ayudar a mantener una conexión, cada stream de Decahose admite [Redundant Connections](/es/x-api/enterprise-gnip-2.0/fundamentals/decahose-api#additional-streams). La arquitectura más común es que un stream tenga dos conexiones y, en el lado del cliente, haya dos consumidores independientes, idealmente en redes diferentes. Con este diseño, puede haber redundancia en las redes del lado del cliente, los servidores y las rutas hacia el almacén de datos. Ten en cuenta que se sirve una copia completa de los datos en cada conexión y que el lado del cliente debe tolerar y gestionar los datos duplicados.
3. Se proporcionará un &quot;heartbeat&quot; cada 10 segundos; sin embargo, con el stream de Decahose, el volumen de datos es lo suficientemente alto como para que incluso una breve duración (por ejemplo, unos segundos) sin Publicaciones pueda indicar un problema de conexión. Por lo tanto, tanto una “ausencia de datos” como la falta de un heartbeat pueden utilizarse para detectar una desconexión.

Dado que las desconexiones ocurrirán, el stream de Decahose tiene una función dedicada de [Recovery](/es/x-api/enterprise-gnip-2.0/fundamentals/decahose-api#overview) y una función de [Backfill](/es/x-api/enterprise-gnip-2.0/fundamentals/decahose-api#backfill) para ayudar a recuperar los datos que se hayan perdido debido a desconexiones y otros problemas operativos.

<div id="additional-streams">
  #### Flujos adicionales
</div>

Contar con flujos Decahose adicionales es otra forma de ayudar a mejorar la fiabilidad de tu solución. Cualquier flujo adicional es completamente independiente y tiene su propio endpoint único. A cada flujo se le asigna su propio stream&#95;label y esta etiqueta, junto con el nombre de tu cuenta, forman parte de la URL de ese flujo. Consulta el siguiente ejemplo:

https://gnip-stream.x.com/stream/sample10/accounts/:account&#95;name/publishers/twitter/:stream&#95;label.json

La convención más común es tener un flujo en tiempo real dedicado para tu sistema de producción y un flujo adicional disponible para desarrollo y pruebas. Contar con un flujo de prueba/desarrollo permite a los clientes de Decahose disponer de un flujo para probar las actualizaciones de sus clientes consumidores. Aunque se puede asignar cualquier etiqueta (única) a un flujo, una convención es usar &quot;prod&quot; para el flujo de producción y &quot;dev&quot; o &quot;sandbox&quot; para un flujo de desarrollo adicional.

La cantidad de flujos y sus etiquetas únicas se puede configurar a través del representante de tu cuenta.

**Conexiones redundantes**

Una conexión redundante simplemente te permite establecer más de una conexión simultánea al flujo de datos. Esto proporciona redundancia al permitirte conectarte al mismo flujo con dos consumidores separados, recibiendo los mismos datos a través de ambas conexiones. Así, tu aplicación tiene una conmutación por error en caliente para varias situaciones, por ejemplo, cuando un flujo se desconecta o cuando falla el servidor principal de tu aplicación.

La cantidad de conexiones permitidas para un flujo determinado se puede configurar a través del representante de tu cuenta. Para usar un flujo redundante, simplemente conéctate a la misma URL que utilizas para tu conexión principal. Los datos de tu flujo se enviarán por ambas conexiones, y ambas conexiones de flujo se representarán en el panel del flujo.

Ten en cuenta que, para fines de facturación, deduplicamos los recuentos de actividad que recibes mediante múltiples conexiones, de modo que solo se te facture una vez por cada actividad única. Dado que Decahose tiene dos particiones, a continuación se muestra un ejemplo de cómo funciona el conteo de conexiones:

Connect to decahose partition=1
Connect to decahose partition=1
Connect to decahose partition=2

La situación anterior arroja un total de tres conexiones: dos conexiones a partition=1 y una conexión a partition=2. Normalmente querrás tener la misma cantidad de conexiones en cada partición, por lo que este ejemplo destaca una situación en la que la conexión redundante a partition=2 se ha caído y quieres investigar más a fondo.

**Recuperación**

<div id="overview">
  #### Descripción general
</div>

Recovery es una herramienta de recuperación de datos (no debe utilizarse para la recopilación primaria de datos) que proporciona acceso en streaming a una ventana continua de 5 días de datos históricos recientes de X. Debe utilizarse para recuperar datos en los casos en que tu aplicación cliente pierda datos en el flujo en tiempo real, ya sea por desconectarse durante un breve periodo o por cualquier otra situación en la que no consigas ingerir datos en tiempo real durante un periodo de tiempo.

<div id="using-recovery">
  #### Uso de Recovery
</div>

Con el stream de Recovery, tu aplicación puede hacer solicitudes que funcionan de la misma manera que las solicitudes a los streams en tiempo real. Sin embargo, tu aplicación debe especificar parámetros en la URL que indiquen la ventana de tiempo que estás solicitando. En otras palabras, una solicitud de Recovery le pide a la API: &quot;Publicaciones desde la hora A hasta la hora B&quot;. Estas Publicaciones se entregan luego a través de tu conexión de streaming de una manera que imita el stream en tiempo real, pero a una velocidad ligeramente inferior al tiempo real. Consulta el siguiente ejemplo:

&quot;https://stream-data-api.x.com/stream/powertrack/accounts/someAccountName/publishers/twitter/powertrack.json?startTime=2023-07-05T17:09:12.070Z&quot;

Las Publicaciones se entregan comenzando por el primer (más antiguo) minuto del período de tiempo especificado y continúan cronológicamente hasta que se entrega el último minuto. En ese momento, se envía a través de la conexión un mensaje de &quot;Recovery Request Completed&quot;, y luego el servidor cierra la conexión. Si tu solicitud comienza en un momento del día en el que se produjeron pocos o ningún resultado coincidente, es probable que transcurra un período de tiempo antes de que se entreguen los primeros resultados: los datos se entregarán cuando Recovery encuentre coincidencias en la parte del archivo que se esté procesando en ese momento. Cuando no haya resultados disponibles para entregar, el stream seguirá enviando caracteres de retorno de carro, o &quot;heartbeats&quot;, a través de la conexión para evitar que se agote el tiempo de espera.

Recovery está pensado como una herramienta para recuperar fácilmente datos perdidos debido a desconexiones cortas, no para períodos de tiempo muy largos como un día completo. Si surge la necesidad de recuperar datos durante períodos prolongados, recomendamos dividir las solicitudes más largas en ventanas de tiempo más cortas (por ejemplo, ventanas de dos horas) para reducir la posibilidad de que se produzca una desconexión a mitad de la solicitud debido a la inestabilidad de internet u otros motivos, y para proporcionar mayor visibilidad sobre el progreso de solicitudes largas.

<div id="data-availability">
  #### Disponibilidad de datos
</div>

Puedes usar la función Recovery para recuperar datos perdidos dentro de las últimas 24 horas si no puedes volver a conectarte dentro de la ventana de backfill de 5 minutos.

La función de recuperación en streaming te permite disponer de una ventana de backfill ampliada de 24 horas. Recovery te permite “recuperar” el período de tiempo de datos perdidos. Se inicia un stream de recuperación cuando realizas una solicitud de conexión usando los parámetros de solicitud `start_time` y `end_time`. Una vez conectado, Recovery volverá a hacer streaming del período de tiempo indicado y luego se desconectará.  

Puedes realizar 2 solicitudes simultáneas a Recovery al mismo tiempo, es decir, “dos trabajos de recuperación”. Recovery funciona técnicamente de la misma manera que el backfill, excepto porque se definen una hora de inicio y una hora de finalización. Un período de recuperación corresponde a un único intervalo de tiempo.

<div id="backfill">
  #### Backfill
</div>

Para solicitar backfill, debes agregar un parámetro backfillMinutes=N a tu solicitud de conexión, donde N es la cantidad de minutos (1-5, solo números enteros) que se van a recuperar cuando se establezca la conexión. Por ejemplo, si te desconectas durante 90 segundos, deberías agregar backfillMinutes=2 a tu solicitud de conexión. Dado que esta solicitud proporcionará backfill durante 2 minutos, incluido el período de 30 segundos antes de que te desconectaras, tu *aplicación consumidora debe tolerar datos duplicados*.

Un ejemplo de URL de solicitud de conexión de Decahose, en el que se solicita un backfill de 5 minutos para la partición 1, es el siguiente:

https://gnip-stream.x.com/stream/sample10/accounts/:account&#95;name/publishers/twitter/:stream&#95;label.json?partition=1&amp;backfillMinutes=5

**NOTAS:**

* Tienes la opción de usar siempre &quot;backfillMinutes=5&quot; cuando te conectas, y luego gestionar cualquier dato duplicado que se proporcione.

* Si estás desconectado por más de cinco minutos, puedes recuperar los datos usando Recovery.

**Recuperarse de una desconexión**

Reiniciar y recuperarse de una desconexión implica varios pasos:

* Determinar la duración del período de desconexión.
  * ¿5 minutos o menos?
    * Si tienes backfill habilitado para el stream, prepara la solicitud de conexión con el parámetro &quot;backfillMinutes&quot; apropiado.
  * ¿Más de 5 minutos?
    * Si tienes un stream de Recovery, realiza una solicitud de Recovery para el período de tiempo desconectado (idealmente con tu conjunto de reglas de tiempo real actual, usando la Rules API si es necesario).
* Solicitar una nueva conexión.

Cuando experimentes desconexiones o tiempos de inactividad, aquí tienes estrategias para mitigar y recuperarte en este escenario:

1. **Implementar backfill**\
   Backfill te permite reconectarte desde un punto anterior a la desconexión de una conexión de stream y cubre desconexiones de hasta 5 minutos. Se implementa incluyendo un parámetro en la solicitud de conexión.

2. **Consumir un stream redundante desde otra ubicación**\
   Si el stream redundante puede enviarse al mismo entorno en vivo, desduplicando los datos, eliminarás la necesidad de recuperación a menos que TANTO el stream normal como el stream redundante experimenten tiempo de inactividad o desconexiones simultáneas. Si el stream redundante no puede transmitirse en vivo al entorno de producción, se puede escribir en un almacén de datos de &quot;emergencia&quot; separado. Luego, en caso de desconexiones o tiempos de inactividad en la conexión de stream primaria, tu sistema tendrá datos disponibles para completar tu base de datos primaria durante el período de tiempo en el que faltan datos.

3. **Implementar Recovery**\
   Cuando las desconexiones o tiempos de inactividad afecten tanto al stream primario como al stream redundante, usa Decahose Recovery para recuperar cualquier dato perdido. La API proporciona una ventana móvil que cubre 5 días del archivo, y se utiliza mejor solicitando no más de una hora de esa ventana a la vez y transmitiéndola. Esto se hace en paralelo al stream en tiempo real. Ten en cuenta que no tenemos soluciones para recuperar datos de Decahose más allá de la ventana de 5 días proporcionada por Recovery, por lo que es importante que utilices un stream redundante para asegurarte de que tengas una copia completa de los datos de tu lado en caso de un tiempo de inactividad significativo en tu entorno.

Cuando detectes volúmenes de datos almacenados anormales:\
Posibles formas de detectar datos faltantes cuando no se produjeron desconexiones o tiempos de inactividad…

1. Contar las Publicaciones entrantes\
   Tu sistema debería contar el número bruto de Publicaciones que recibes al inicio de tu aplicación de ingesta y luego proporcionar una forma de comparar esos números con el número de Publicaciones que llega a tu almacén de datos final. Cualquier diferencia se puede supervisar y usar para alertar a tu equipo sobre problemas que causen la pérdida de datos después de que se hayan recibido.

2. Analizar volúmenes almacenados anormales\
   También puedes analizar los volúmenes de datos almacenados en tu base de datos final para buscar caídas anormales. Esto también puede indicar problemas, aunque probablemente habrá circunstancias en las que las caídas de volumen sean normales (por ejemplo, si la plataforma X no está disponible y la gente no puede crear Publicaciones durante algún período de tiempo).

<div id="api-reference">
  ## Referencia de la API
</div>

<div id="decahose-stream">
  ### Flujo de Decahose
</div>

Saltar a en esta página

[Métodos](/es/x-api/enterprise-gnip-2.0/fundamentals/decahose-api#methods)

[Autenticación](/es/x-api/enterprise-gnip-2.0/fundamentals/decahose-api#authentication)

[GET /&#123;stream-type&#125;/:stream](/es/x-api/enterprise-gnip-2.0/fundamentals/decahose-api#get-stream-type-stream)

[API de reproducción](/es/x-api/enterprise-gnip-2.0/fundamentals/decahose-api#replay-api)

<div id="methods">
  #### Métodos
</div>

| Método | Descripción |
| :--- | :--- |
| [GET /&#123;stream-type&#125;/:stream](#Stream) | Conectar con el flujo de datos |

<div id="authentication">
  #### Autenticación[](#authentication- "Enlace permanente a este encabezado")
</div>

Todas las solicitudes a las APIs de Volume Stream deben usar autenticación HTTP básica (`HTTP Basic Authentication`), construida a partir de una combinación válida de dirección de correo electrónico y contraseña utilizada para iniciar sesión en tu cuenta en console.gnip.com. Las credenciales deben enviarse como el encabezado `Authorization` en cada solicitud. Por lo tanto, confirma que tu cliente agregue el encabezado HTTP `"Authentication: Basic"` (con las credenciales codificadas sobre HTTPS) a todas las solicitudes de la API.

#### GET {stream-type}:stream

Establece una conexión persistente con el stream Firehose, mediante la cual se entregarán los datos en tiempo real.

<div id="request-specifications">
  #### Especificaciones de la solicitud
</div>

|     |     |
| :--- | :--- |
| **Método de solicitud** | HTTP GET |
| **Tipo de conexión** | Keep-Alive  <br />  <br />Esto se debe especificar en el encabezado de la solicitud. |
| **URL** | Se encuentra en la página de Ayuda de la API del stream en tu panel, utilizando la siguiente estructura:  <br />  <br />Decahose:<br /><br />[https://gnip-stream.x.com/stream/sample10/accounts/:account&#95;name/publishers/twitter/:stream&#95;label.json?partition=1](https://gnip-stream.x.com/stream/sample10/accounts/:account_name/publishers/twitter/:stream_label.json?partition=1) |
| **Partición (obligatoria)** | `partition=\{#}` - Ahora es obligatorio usar particiones para consumir el stream completo. Deberás conectarte al stream con el parámetro de partición especificado. A continuación se muestra el número de particiones por stream:<br /><br />* Decahose: 2 particiones |
| **Compresión** | Gzip. Para conectarte al stream usando compresión Gzip, simplemente envía un encabezado Accept-Encoding en la solicitud de conexión. El encabezado debe verse como sigue:  <br />  <br />Accept-Encoding: gzip |
| **Codificación de caracteres** | UTF-8 |
| **Formato de la respuesta** | JSON. El encabezado de tu solicitud debe especificar el formato JSON para la respuesta. |
| **Límite de solicitudes** | 10 solicitudes cada 60 segundos. |
| **Parámetro de Backfill** | Si has adquirido un stream con Backfill habilitado, deberás agregar el parámetro &quot;backfillMinutes&quot; en la solicitud GET para habilitarlo. |
| **Tiempo de espera de lectura** | Configura un tiempo de espera de lectura en tu cliente y asegúrate de que esté establecido en un valor superior a 30 segundos. |
| **Compatibilidad con edición de Publicaciones** | Todos los objetos de Publicación incluirán metadatos de edición que describen el historial de edición de la publicación. Consulta la página de conceptos básicos [&quot;Edit Tweets&quot;](/es/x-api/enterprise-gnip-2.0/fundamentals/edit-tweets) para obtener más detalles. |

<div id="responses">
  #### Respuestas
</div>

Las siguientes respuestas pueden ser devueltas por la API para estas solicitudes. La mayoría de los códigos de error se devuelven con una cadena de texto con información adicional en el cuerpo. Para respuestas que no sean 200, los clientes deben intentar volver a conectarse.

| Status | Text | Description |
| :--- | :--- | :--- |
| 200 | Success | La conexión se abrió correctamente y se enviarán nuevas actividades a medida que vayan llegando. |
| 401 | Unauthorized | La autenticación HTTP falló debido a credenciales no válidas. Inicia sesión en console.gnip.com con tus credenciales para asegurarte de que las estás utilizando correctamente en tu solicitud. |
| 406 | Not Acceptable | Generalmente, esto ocurre cuando tu cliente no incluye correctamente los encabezados para aceptar la codificación gzip desde el stream, pero también puede ocurrir en otras circunstancias.  <br />  <br />Contendrá un mensaje JSON similar a: &quot;Esta conexión requiere compresión. Para habilitar la compresión, envía un encabezado &#39;Accept-Encoding: gzip&#39; en tu solicitud y prepárate para descomprimir el stream a medida que se lea en el cliente&quot;. |
| 429 | Rate Limited | Tu aplicación ha superado el límite de solicitudes de conexión. |
| 503 | Service Unavailable | Problema en el servidor de Twitter. Vuelve a conectarte utilizando un patrón de retroceso exponencial. Si no se ha publicado ningún aviso sobre este problema en la [X API Status Page](https://api.twitterstat.us/), contacta con soporte o con el servicio de emergencias si no puedes conectarte después de 10 minutos. |

<div id="example-curl-request">
  #### Ejemplo de solicitud cURL
</div>

La siguiente solicitud de ejemplo se realiza con cURL desde la línea de comandos. Ten en cuenta que estas solicitudes también se pueden enviar con el lenguaje de programación de tu preferencia:

```
curl --compressed -v -uexample@customer.com "https://gnip-stream.x.com/stream/firehose/accounts/:account\_name/publishers/twitter/:stream\_label.json?partition={#}"
```

<div id="replay-api">
  #### API de Replay
</div>

La API de Replay es un complemento importante de los streams de volumen en tiempo real. Replay es una herramienta de recuperación de datos que proporciona acceso en streaming a una ventana deslizante de datos históricos recientes de X.