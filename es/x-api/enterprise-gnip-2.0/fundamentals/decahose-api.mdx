---
title: API de Decahose
keywords: ["Decahose", "flujo del 10 %", "API de Decahose", "Enterprise Decahose", "flujo muestreado del 10 %", "streaming Enterprise"]
---

<Note>
  Este endpoint se ha actualizado para incluir metadatos de edición de Publicaciones. Obtén más información sobre estos metadatos en la [página de conceptos básicos de &quot;Editar Publicaciones&quot;](/es/x-api/enterprise-gnip-2.0/fundamentals/edit-tweets). 
</Note>

<div id="decahose-stream">
  ### Flujo de Decahose
</div>

[Enterprise](https://developer.x.com/en/products/x-api/enterprise)

*Esta es una API Enterprise disponible solo dentro de nuestros niveles de acceso gestionados. Para usar esta API, primero debes crear una cuenta con nuestro equipo de ventas de Enterprise. [Más información](https://developer.x.com/en/products/x-api/enterprise)*

Decahose proporciona una muestra aleatoria del 10% del X Firehose en tiempo real a través de una conexión de streaming. Esto se logra mediante un algoritmo de muestreo en tiempo real que selecciona los datos de forma aleatoria, al mismo tiempo que permite la entrega de datos con la baja latencia esperada a medida que X los envía a través del Firehose.

A continuación, se muestran algunas de las características disponibles con Decahose:

* **URL ampliadas y mejoradas:** - desarrolla por completo las URL acortadas y proporciona metadatos adicionales (título y descripción de la página)
* **Partición del stream** - 2 particiones, cada una con el 50% del volumen del flujo de Decahose
* **Mayor fiabilidad** - diversidad geográfica de los sistemas de backend

Nota: Estos datos se entregan en bloque y no admiten filtrado adicional (por ejemplo, por palabras clave).

`ENTERPRISE`

<div id="streaming-likes">
  ### Streaming de Me gusta
</div>

*Esta es una API de Enterprise disponible solo dentro de nuestros niveles de acceso gestionados. Para usar esta API, primero debes configurar una cuenta con nuestro equipo de ventas de Enterprise. [Más información](https://developer.x.com/en/products/x-api/enterprise)*

Los Me gusta permiten saber quién indica que le gustan las Publicaciones y proporcionan recuentos precisos de Me gusta. Firehose y Decahose de Gnip pueden entregar Me gusta públicos relacionados con las Publicaciones entregadas a través de Gnip. Esto proporciona métricas de interacción pública y de audiencia en tiempo real asociadas a una Publicación.
 

**Primeros pasos con Me gusta**

Al prepararte para consumir datos de Me gusta, debes saber que:

* Los Me gusta se entregan a través de un flujo independiente y separado
* Históricamente, los Me gusta se denominan “Favorites”. La carga útil enriquecida en formato nativo mantiene esta nomenclatura
* Los flujos incluyen solo Me gusta públicos
  * Público significa que el usuario que da Me gusta, el creador de la Publicación y la Publicación son todos públicos en la plataforma
* Los Me gusta son muy similares a los Retweets y representan una señal pública de interacción
* Los elementos de la carga útil incluyen:
  * Objeto de la Publicación original
  * Objeto del actor que creó la Publicación original
  * Objeto del actor que realizó la acción de Me gusta
* Solo se puede dar Me gusta al contenido original
  * No se puede dar Me gusta a los Retweets. Un Me gusta de un Retweet se aplica a la Publicación original
  * Las Publicaciones citadas (Quoted Tweets) *se pueden* indicar con Me gusta
* Las actividades de Me gusta incluyen las Gnip Enrichments aplicables (cuando se hayan comprado/aplicado)
* Productos / funciones compatibles
  * Los flujos de Me gusta son compatibles con Backfill (cuando se haya comprado/aplicado)
  * No hay compatibilidad con Replay para los flujos de Me gusta
  * No hay compatibilidad con Search ni Historical para los Me gusta
  * No hay planes inmediatos para añadir compatibilidad con Me gusta a PowerTrack

**Decahose**

* Para el 10 % de Publicaciones de muestra entregadas en Decahose, el flujo incluye el 100 % de los Me gusta públicos aplicables
* **Particiones:** 2
* **Estructura de URL**
  * https://gnip-stream.x.com/stream/sample10-likes/accounts/&lt;accountName&gt;/publishers/twitter/&lt;streamLabel&gt;.json?partition=1

**Carga útil en formato nativo enriquecido**

```json
{
   "id":"43560406e0ad9f68374445f5f30c33fc",
   "created_at":"Thu Dec 01 22:27:39 +0000 2016",
   "timestamp_ms":1480631259353,
   "favorited_status":{
      "created_at":"Thu Dec 01 22:27:16 +0000 2016",
      "id":804451830033948672,
      "id_str":"804451830033948672",
      "text":"@kafammheppduman",
      "source":"\u003ca href=\"http:\/\/x.com\/download\/android\" rel=\"nofollow\"\u003eX para Android\u003c\/a\u003e",
      "truncated":false,
      "in_reply_to_status_id":803694205163814912,
      "in_reply_to_status_id_str":"803694205163814912",
      "in_reply_to_user_id":2855759795,
      "in_reply_to_user_id_str":"2855759795",
      "in_reply_to_screen_name":"kafammheppduman",
      "user":{
         "id":2855759795,
         "id_str":"2855759795",
         "name":"delirdim kanka",
         "screen_name":"kafammheppduman",
         "location":"sanane",
         "url":"http:\/\/instagram.com\/kafammheppduman",
         "description":"Manit @GalatasaraySk \ud83d\udc9e",
         "translator_type":"none",
         "protected":false,
         "verified":false,
         "followers_count":3702,
         "friends_count":607,
         "listed_count":1,
         "favourites_count":113338,
         "statuses_count":389,
         "created_at":"Sat Nov 01 22:38:25 +0000 2014",
         "utc_offset":null,
         "time_zone":null,
         "geo_enabled":true,
         "lang":"tr",
         "contributors_enabled":false,
         "is_translator":false,
         "profile_background_color":"C0DEED",
         "profile_background_image_url":"",
         "profile_background_image_url_https":"",
         "profile_background_tile":false,
         "profile_link_color":"1DA1F2",
         "profile_sidebar_border_color":"C0DEED",
         "profile_sidebar_fill_color":"DDEEF6",
         "profile_text_color":"333333",
         "profile_use_background_image":true,
       "Profile_image_url": "http:\/\/pbs.twimg.com\/profile_images\/804421763945861121\/v3bp9pnq_normal.jpg",
         "Profile_image_url_https": "https:\/\/pbs.twimg.com\/profile_images\/804421763945861121\/v3bp9pnq_normal.jpg",
         "profile_banner_url":"https:\/\/pbs.twimg.com\/profile_banners\/2855759795\/1480630085",
         "default_profile":true,
         "default_profile_image":false,
         "following":null,
         "follow_request_sent":null,
         "notifications":null
      },
      "geo":null,
      "coordinates":null,
      "place":null,
      "contributors":null,
      "is_quote_status":false,
      "retweet_count":0,
      "favorite_count":0,
      "entities":{
         "hashtags":[],
         "urls":[],
         "user_mentions":[
            {
               "screen_name":"kafammheppduman",
               "name":"delirdim kanka",
               "id":2855759795,
               "id_str":"2855759795",
               "indices":[
                  0,
                  16
               ]
            }
         ],
         "symbols":[]
      },
      "favorited":false,
      "retweeted":false,
      "filter_level":"low",
      "lang":"und"
   },
   "user":{
      "id":774146932365070336,
      "id_str":"774146932365070336",
      "name":"Uyuyan Adam",
      "screen_name":"saykoMenn",
      "location":"Tarsus, T\u00fcrkiye",
      "url":"http:\/\/connected2.me\/pmc1i",
      "description":null,
      "translator_type":"none",
      "protected":false,
      "verified":false,
      "followers_count":414,
      "friends_count":393,
      "listed_count":0,
      "favourites_count":9868,
      "statuses_count":370,
      "created_at":"Fri Sep 09 07:26:26 +0000 2016",
      "utc_offset":null,
      "time_zone":null,
      "geo_enabled":false,
      "lang":"tr",
      "contributors_enabled":false,
      "is_translator":false,
      "profile_background_color":"F5F8FA",
      "profile_background_image_url":"",
      "profile_background_image_url_https":"",
      "profile_background_tile":false,
      "profile_link_color":"1DA1F2",
      "profile_sidebar_border_color":"C0DEED",
      "profile_sidebar_fill_color":"DDEEF6",
      "profile_text_color":"333333",
      "profile_use_background_image":true,
      "Profile_image_url": "http:\/\/pbs.twimg.com\/profile_images\/802992813424201728\/VMzcTL3x_normal.jpg",
      "Profile_image_url_https": "https:\/\/pbs.twimg.com\/profile_images\/802992813424201728\/VMzcTL3x_normal.jpg",
      "profile_banner_url":"https:\/\/pbs.twimg.com\/profile_banners\/774146932365070336\/1480283382",
      "default_profile":true,
      "default_profile_image":false,
      "following":null,
      "follow_request_sent":null,
      "notifications":null
   }
}
```

**Carga útil de eliminación de Me gusta / «Quitar Me gusta»**

```json
{
   "delete":{
      "favorite":{
         "tweet_id":696615514970279937,
         "tweet_id_str":"696615514970279937",
         "user_id":2510287578,
         "user_id_str":"2510287578"
      },
      "timestamp_ms":"1480437031205"
   }
}
```

<div id="guides">
  ## Guías
</div>

<div id="recovery-and-redundency">
  ### Recuperación y redundancia
</div>

**Introducción** 

Al transmitir grandes volúmenes de Publicaciones en tiempo real, existe un conjunto de prácticas recomendadas que promueven tanto la fiabilidad de los datos como su fidelidad íntegra. Al consumir datos en tiempo real, maximizar el tiempo de conexión es un objetivo fundamental. Cuando se producen desconexiones, es importante detectarlas automáticamente y reconectar. Después de reconectar, es importante evaluar si hay períodos para los que se deba realizar backfill de datos. El componente que gestiona estos detalles y consume Publicaciones en tiempo real es solo una parte de un sistema con consideraciones de red, almacén de datos, servidor y almacenamiento. Dada la complejidad de estos sistemas, otra práctica recomendada es disponer de diferentes entornos de streaming, con al menos flujos separados para desarrollo/pruebas y producción.

Decahose incluye un conjunto de funcionalidades que ayudan a conseguir estos objetivos.

1. Para admitir múltiples entornos, podemos implementar [Additional Streams](#AdditionalStreams) para tu cuenta. Estos flujos son independientes entre sí y tienen una `stream_label` diferente para ayudar a diferenciarlos.
2. Para ayudar a mantener una conexión, cada flujo de Decahose es compatible con [Redundant Connections](/es/x-api/enterprise-gnip-2.0/fundamentals/decahose-api#additional-streams). La arquitectura más común es que un flujo tenga dos conexiones y, en el lado del cliente, haya dos consumidores independientes, idealmente en diferentes redes. Con este diseño, puede haber redundancia en las redes del lado del cliente, los servidores y las rutas hacia el almacén de datos. Ten en cuenta que en cada conexión se sirve una copia completa de los datos y que el cliente debe ser tolerante a los datos duplicados y gestionarlos.
3. Se proporcionará un &#39;**heartbeat**&#39; cada 10 segundos; sin embargo, con el flujo de Decahose, el volumen de datos es lo suficientemente alto como para que incluso una duración pequeña (por ejemplo, unos pocos segundos) sin Publicaciones pueda indicar un problema de conexión. Por lo tanto, tanto un &#39;silencio de datos&#39; como la falta de un heartbeat pueden utilizarse para detectar una desconexión.

Dado que las desconexiones ocurrirán, el flujo de Decahose tiene una funcionalidad dedicada de [Recovery](/es/x-api/enterprise-gnip-2.0/fundamentals/decahose-api#overview) y de [Backfill](/es/x-api/enterprise-gnip-2.0/fundamentals/decahose-api#backfill) para ayudar a recuperar los datos que se perdieron debido a desconexiones y otros problemas operativos.

<div id="additional-streams">
  #### Streams adicionales
</div>

Tener streams adicionales de Decahose es otra forma de ayudar a incorporar confiabilidad en tu solución. Cualquier stream adicional es completamente independiente y tiene su propio endpoint único. A cada stream se le asigna su propio stream&#95;label, y esta etiqueta, junto con el nombre de tu cuenta, forman parte de la URL de ese stream. Consulta el siguiente ejemplo:

https://gnip-stream.x.com/stream/sample10/accounts/:account&#95;name/publishers/twitter/:stream&#95;label.json

La convención más común es tener un stream en tiempo real dedicado a tu sistema de producción y un stream adicional disponible para desarrollo y pruebas. Contar con un stream de prueba/desarrollo permite a los clientes de Decahose disponer de un stream para probar actualizaciones de sus consumidores. Aunque se puede asignar cualquier etiqueta (única) a un stream, una convención es usar &#39;prod&#39; para el stream de producción y &#39;dev&#39; o &#39;sandbox&#39; para un stream adicional de desarrollo.

La cantidad de streams y sus etiquetas únicas se puede configurar a través de tu representante de cuenta.

**Conexiones redundantes**

Una conexión redundante simplemente te permite establecer más de una conexión simultánea al stream de datos. Esto proporciona redundancia al permitirte conectarte al mismo stream con dos consumidores diferentes, recibiendo los mismos datos a través de ambas conexiones. Así, tu aplicación cuenta con un failover en caliente para varias situaciones, por ejemplo, cuando un stream se desconecta o cuando falla el servidor principal de tu aplicación.

La cantidad de conexiones permitidas para un stream dado se puede configurar a través de tu representante de cuenta. Para usar un stream redundante, simplemente tienes que conectarte a la misma URL que utilizas para tu conexión principal. Los datos de tu stream se enviarán a través de ambas conexiones, y ambas conexiones del stream aparecerán representadas en el panel del stream.

Ten en cuenta que, para fines de facturación, desduplicamos los recuentos de actividad que recibes a través de múltiples conexiones, de modo que solo se te facture una vez por cada actividad única. Dado que Decahose tiene dos particiones, a continuación se muestra un ejemplo de cómo funciona el conteo de conexiones:

Connect to decahose partition=1\
Connect to decahose partition=1\
Connect to decahose partition=2

La situación anterior da como resultado un total de tres conexiones: dos conexiones a partition=1 y una conexión a partition=2. Normalmente, querrías tener la misma cantidad de conexiones para cada partición, por lo que este ejemplo indica una situación en la que la conexión redundante a partition=2 se ha caído y deseas investigarla más a fondo.

**Recuperación**

<div id="overview">
  #### Descripción general
</div>

Recovery es una herramienta de recuperación de datos (que no debe utilizarse para la recopilación principal de datos) que proporciona acceso por streaming a una ventana móvil de 5 días de datos históricos recientes de X. Debe utilizarse para recuperar datos en escenarios en los que tu aplicación cliente pierda datos del stream en tiempo real, ya sea debido a una desconexión durante un breve período o por cualquier otro motivo que te impida ingerir datos en tiempo real durante un intervalo de tiempo.

<div id="using-recovery">
  #### Using Recovery
</div>

Con el stream de Recovery, tu aplicación puede hacer solicitudes que funcionan de la misma manera que las solicitudes a los streams en tiempo real. Sin embargo, tu aplicación debe especificar parámetros en la URL que indiquen la ventana de tiempo que estás solicitando. En otras palabras, una solicitud de Recovery le pide a la API &quot;Publicaciones desde la hora A hasta la hora B&quot;. Estas publicaciones se entregan a través de tu conexión de streaming de una manera que imita el stream en tiempo real, pero a una velocidad ligeramente inferior al tiempo real. Consulta el siguiente ejemplo:

&quot;https://stream-data-api.x.com/stream/powertrack/accounts/someAccountName/publishers/twitter/powertrack.json?startTime=2023-07-05T17:09:12.070Z&quot;

Las publicaciones se entregan comenzando por el primer (más antiguo) minuto del período de tiempo especificado y continúan cronológicamente hasta que se entrega el último minuto. En ese momento, se envía a través de la conexión un mensaje Recovery Request Completed y, a continuación, el servidor cierra la conexión. Si tu solicitud comienza en una hora del día en la que se produjeron pocos o ningún resultado coincidente, probablemente transcurra cierto tiempo antes de que se entreguen los primeros resultados: los datos se entregarán cuando Recovery encuentre coincidencias en la parte del archivo que esté procesando en ese momento. Cuando no haya resultados disponibles para entregar, el stream seguirá enviando retornos de carro, o &quot;heartbeats&quot;, a través de la conexión para evitar que se agote el tiempo de espera.

Recovery está pensado como una herramienta para recuperar fácilmente datos perdidos debido a desconexiones cortas, no para períodos de tiempo muy largos, como un día completo. Si surge la necesidad de recuperar datos durante períodos largos, recomendamos dividir las solicitudes más extensas en ventanas de tiempo más cortas (por ejemplo, de dos horas) para reducir la posibilidad de que se produzca una desconexión a mitad de la solicitud debido a la volatilidad de Internet u otras razones, y para proporcionar mayor visibilidad sobre el progreso de solicitudes largas.

<div id="data-availability">
  #### Disponibilidad de datos
</div>

Puedes usar la función Recovery para recuperar datos perdidos de las últimas 24 horas si no puedes reconectarte dentro de la ventana de backfill de 5 minutos.

La función de recuperación por streaming te permite disponer de una ventana de backfill ampliada de 24 horas. Recovery te permite “recuperar” el período de tiempo de datos perdidos. Se inicia un recovery stream cuando haces una solicitud de conexión usando los parámetros de solicitud `start_time` y `end_time`. Una vez conectado, Recovery volverá a transmitir el período de tiempo indicado y luego se desconectará.

Podrás realizar 2 solicitudes simultáneas a Recovery al mismo tiempo, es decir, “dos trabajos de Recovery”. Recovery funciona técnicamente de la misma manera que backfill, excepto que se define una hora de inicio y una hora de fin. Un período de Recovery corresponde a un único intervalo de tiempo.

<div id="backfill">
  #### Backfill
</div>

Para solicitar backfill, necesitas agregar un parámetro backfillMinutes=N a tu solicitud de conexión, donde N es la cantidad de minutos (1-5, solo números enteros) que se va a recuperar cuando se establezca la conexión. Por ejemplo, si te desconectas durante 90 segundos, debes agregar backfillMinutes=2 a tu solicitud de conexión. Dado que esta solicitud proporcionará backfill durante 2 minutos, incluido el período de 30 segundos antes de que te desconectaras, tu *aplicación consumidora debe tolerar datos duplicados*.

Un ejemplo de URL de solicitud de conexión de Decahose, que solicita un backfill de 5 minutos para la partición 1, es el siguiente:

https://gnip-stream.x.com/stream/sample10/accounts/:account&#95;name/publishers/twitter/:stream&#95;label.json?partition=1&amp;backfillMinutes=5

**NOTAS:**

* Sí tienes la opción de usar siempre ‘backfillMinutes=5’ cuando te conectes y luego manejar cualquier dato duplicado que se proporcione.

* Si estás desconectado durante más de cinco minutos, puedes recuperar datos usando Recovery.

**Recuperarse de una desconexión**

Reiniciar y recuperarse de una desconexión implica varios pasos:

* Determinar la duración del período de desconexión.
  * ¿5 minutos o menos?
    * Si tienes Backfill habilitado para el stream, prepara la solicitud de conexión con el parámetro ‘backfillMinutes’ adecuado.
  * ¿Más de 5 minutos?
    * Si tienes un stream de Recovery, realiza una solicitud de Recovery para el período de tiempo desconectado (idealmente con tu conjunto de reglas de tiempo real actual, usando la Rules API si es necesario).
* Solicitar una nueva conexión.

Cuando experimentes desconexiones o tiempo de inactividad, estas son estrategias para mitigar y recuperarte en este escenario:

1. **Implementar backfill**\
   El backfill te permite reconectarte desde un punto anterior a la desconexión de una conexión de stream y cubre desconexiones de hasta 5 minutos. Se implementa incluyendo un parámetro en la solicitud de conexión.

2. **Consumir un stream redundante desde otra ubicación**\
   Si el stream redundante se puede enviar al mismo entorno en vivo, desduplicando los datos, eliminarás la necesidad de recuperación a menos que TANTO el stream normal como el stream redundante experimenten tiempo de inactividad o desconexiones simultáneas. Si el stream redundante no se puede enviar en vivo al entorno de producción, se puede escribir en un almacén de datos “de emergencia” separado. Luego, en caso de desconexiones o tiempo de inactividad en la conexión del stream principal, tu sistema tendrá datos disponibles para completar tu base de datos principal durante el período de tiempo en el que falten datos.

3. **Implementar Recovery**\
   Cuando las desconexiones o el tiempo de inactividad afecten tanto al stream principal como al stream redundante, usa Decahose Recovery para recuperar cualquier dato perdido. La API proporciona una ventana móvil que cubre 5 días del archivo y se aprovecha mejor solicitando no más de una hora de esa ventana a la vez y haciendo streaming de esos datos. Esto se hace en paralelo al stream de tiempo real. Ten en cuenta que no tenemos soluciones para recuperar datos de Decahose anteriores a la ventana de 5 días que proporciona Recovery, por lo que es importante que utilices un stream redundante para asegurarte de tener una copia completa de los datos de tu lado en caso de un tiempo de inactividad significativo de tu lado.

Cuando detectes volúmenes de datos almacenados anómalos\
Posibles formas de detectar datos faltantes cuando no se produjeron desconexiones ni tiempo de inactividad…

1. Contar las Publicaciones entrantes\
   Tu sistema debe contar el número bruto de Publicaciones que recibes al principio de tu aplicación de ingesta y luego proporcionar una forma de comparar esos números con el número de Publicaciones que llega a tu almacén de datos final. Cualquier diferencia se puede monitorear y usar para alertar a tu equipo sobre problemas que estén causando que se descarten datos después de que se reciban.

2. Analizar volúmenes almacenados anómalos\
   También es posible que quieras analizar los volúmenes de datos almacenados en tu base de datos final para buscar caídas anormales. Esto también puede indicar problemas, aunque probablemente habrá circunstancias en las que las caídas de volumen sean normales (por ejemplo, si la plataforma de X no está disponible y las personas no pueden crear Publicaciones durante algún período de tiempo).

<div id="api-reference">
  ## Referencia de la API
</div>

<div id="decahose-stream">
  ### Flujo de Decahose
</div>

Ir a en esta página:

[Métodos](/es/x-api/enterprise-gnip-2.0/fundamentals/decahose-api#methods)

[Autenticación](/es/x-api/enterprise-gnip-2.0/fundamentals/decahose-api#authentication)

[GET /&#123;stream-type&#125;/:stream](/es/x-api/enterprise-gnip-2.0/fundamentals/decahose-api#get-stream-type-stream)

[API de Replay](/es/x-api/enterprise-gnip-2.0/fundamentals/decahose-api#replay-api)

<div id="methods">
  #### Métodos
</div>

| Método | Descripción |
| :--- | :--- |
| [GET /&#123;stream-type&#125;/:stream](#Stream) | Conectarse al flujo de datos |

<div id="authentication">
  #### Autenticación[](#authentication- "Enlace permanente a este encabezado")
</div>

Todas las solicitudes a las API de Volume Stream deben usar autenticación básica HTTP (HTTP Basic Authentication), construida a partir de una combinación válida de dirección de correo electrónico y contraseña utilizada para iniciar sesión en tu cuenta en console.gnip.com. Las credenciales deben enviarse en el encabezado Authorization en cada solicitud. Por lo tanto, confirma que tu cliente está agregando el encabezado HTTP &quot;Authorization: Basic&quot; (con las credenciales codificadas mediante HTTPS) a todas las solicitudes a la API.

#### GET {stream-type}:stream

Establece una conexión persistente al flujo Firehose, a través de la cual se enviarán los datos en tiempo real.

<div id="request-specifications">
  #### Especificaciones de la solicitud
</div>

|     |     |
| :--- | :--- |
| **Request Method** | HTTP GET |
| **Connection Type** | Keep-Alive  <br />  <br />Esto debe especificarse en el encabezado de la solicitud. |
| **URL** | Se encuentra en la página de ayuda de la API del stream en tu panel de control, utilizando la siguiente estructura:  <br />  <br />Decahose:<br /><br />[https://gnip-stream.x.com/stream/sample10/accounts/:account&#95;name/publishers/twitter/:stream&#95;label.json?partition=1](https://gnip-stream.x.com/stream/sample10/accounts/:account_name/publishers/twitter/:stream_label.json?partition=1) |
| **Partition (required)** | `partition=\{#}` - Ahora la partición es obligatoria para consumir el stream completo. Necesitarás conectarte al stream con el parámetro de partición especificado. A continuación se indica la cantidad de particiones por stream:<br /><br />* Decahose: 2 particiones |
| **Compression** | Gzip. Para conectarte al stream utilizando compresión Gzip, simplemente envía un encabezado Accept-Encoding en la solicitud de conexión. El encabezado debe tener el siguiente aspecto:  <br />  <br />Accept-Encoding: gzip |
| **Character Encoding** | UTF-8 |
| **Response Format** | JSON. El encabezado de tu solicitud debe especificar el formato JSON para la respuesta. |
| **Rate Limit** | 10 solicitudes cada 60 segundos. |
| **Backfill Parameter** | Si has adquirido un stream con Backfill habilitado, tendrás que añadir el parámetro &quot;backfillMinutes&quot; a la solicitud GET para habilitarlo. |
| **Read Timeout** | Configura un tiempo de espera de lectura en tu cliente y asegúrate de que esté establecido en un valor superior a 30 segundos. |
| **Support for Tweet edits** | Todos los objetos de Publicación incluirán metadatos de edición que describen el historial de ediciones de la Publicación. Consulta la página de fundamentos [&quot;Edit Tweets&quot;](/es/x-api/enterprise-gnip-2.0/fundamentals/edit-tweets) para obtener más detalles. |

<div id="responses">
  #### Respuestas
</div>

Las siguientes respuestas pueden ser devueltas por la API para estas solicitudes. La mayoría de los códigos de error se devuelven con una cadena de texto con detalles adicionales en el cuerpo de la respuesta. Para respuestas distintas de 200, los clientes deben intentar volver a conectarse.

| Status | Text | Description |
| :--- | :--- | :--- |
| 200 | Success | La conexión se abrió correctamente y se enviarán nuevas actividades a medida que lleguen. |
| 401 | Unauthorized | La autenticación HTTP falló debido a credenciales no válidas. Inicia sesión en console.gnip.com con tus credenciales para asegurarte de que las estás usando correctamente en tu solicitud. |
| 406 | Not Acceptable | Generalmente, esto ocurre cuando tu cliente no incluye correctamente los encabezados necesarios para aceptar la codificación gzip del flujo, aunque también puede ocurrir en otras circunstancias.  <br />  <br />Contendrá un mensaje JSON similar a «Esta conexión requiere compresión. Para habilitar la compresión, envía un encabezado &#39;Accept-Encoding: gzip&#39; en tu solicitud y prepárate para descomprimir el flujo a medida que se lea en el lado del cliente». |
| 429 | Rate Limited | Tu aplicación ha superado el límite de solicitudes de conexión. |
| 503 | Service Unavailable | Problema en el servidor de X. Vuelve a conectar utilizando un patrón de reintentos con retroceso exponencial. Si no se ha publicado ningún aviso sobre este problema en la [X API Status Page](https://api.twitterstat.us/), contacta con el soporte o con el soporte de emergencia si no puedes conectarte después de 10 minutos. |

<div id="example-curl-request">
  #### Ejemplo de solicitud cURL
</div>

La siguiente solicitud de ejemplo se realiza con cURL desde la línea de comandos. Ten en cuenta, sin embargo, que también puedes enviar estas solicitudes con el lenguaje de programación de tu preferencia:

```
curl --compressed -v -uexample@customer.com "https://gnip-stream.x.com/stream/firehose/accounts/:account\_name/publishers/twitter/:stream\_label.json?partition={#}"
```

<div id="replay-api">
  #### API Replay
</div>

La API Replay es un complemento importante de los flujos de volumen en tiempo real. Replay es una herramienta de recuperación de datos que proporciona acceso de streaming a una ventana deslizante de datos históricos recientes de X.